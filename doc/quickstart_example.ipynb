{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climsim_utils.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPRODUCTION STEP: Needed to add path from current working directory to the parent directory of the ClimSim package\n",
    "path_to_climslim = \"../../Project3-Resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = path_to_climslim + 'ClimSim/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = path_to_climslim + 'ClimSim/preprocessing/normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')\n",
    "\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale)\n",
    "\n",
    "# set variables to V1 subset\n",
    "data.set_to_v1_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge II (Oct 22, 2023 - Oct 30, 2023)** Machine Learning exploration challenge.\n",
    "\n",
    "### Data loader: The quickstart demo data set was proepared using the preprocessing/create_npy_data_splits.ipynb notebook. This notebook reads in data files from a local folder. \n",
    "\n",
    "Completed by Nashita Rahman\n",
    "\n",
    "1. Could this be revised to read data files from the Huggingface hub? \n",
    "2. Could a user select data by specifying time range and subsampling scheme?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. READING DATAFILES FROM THE HUGGINGFACE HUB\n",
    "\n",
    "Steps:\n",
    "1. Install the python library [huggingface_hub](https://huggingface.co/docs/huggingface_hub/index)\n",
    "2. Import necessary packages for data loading\n",
    "3. Use ``np.load``, hf_hub_download, the repository name (LEAP/subsampled_low_res), the repository type (dataset to account for LFS), and the specific file name to load the files in as needed\n",
    "\n",
    "Changes to starter code:\n",
    "\n",
    "1. New load statements (step 3)\n",
    "\n",
    "```\n",
    "scoring_input = np.load(hf_hub_download(repo_id=repo_name, filename=\"scoring_input.npy\", repo_type=repo_type))\n",
    "scoring_target = np.load(hf_hub_download(repo_id=repo_name, filename=\"scoring_target.npy\", repo_type=repo_type))\n",
    "train_input = np.load(hf_hub_download(repo_id=repo_name, filename=\"train_input.npy\", repo_type=repo_type))\n",
    "train_target = np.load(hf_hub_download(repo_id=repo_name, filename=\"train_target.npy\", repo_type=repo_type))\n",
    "val_input = np.load(hf_hub_download(repo_id=repo_name, filename=\"val_input.npy\", repo_type=repo_type))\n",
    "val_target = np.load(hf_hub_download(repo_id=repo_name, filename=\"val_target.npy\", repo_type=repo_type))\n",
    "```\n",
    "\n",
    "2. Loading in training and validation data\n",
    "\n",
    "```\n",
    "# CHANGE: COMMENTED OUT\n",
    "# data_path = '../../Project3-Resources/subsampled_low_res/'\n",
    "\n",
    "# train_input_path = data_path + 'train_input.npy'\n",
    "# train_target_path = data_path + 'train_target.npy'\n",
    "# val_input_path = data_path + 'val_input.npy'\n",
    "# val_target_path = data_path + 'val_target.npy'\n",
    "\n",
    "# data.input_train = data.load_npy_file(train_input_path)\n",
    "# data.target_train = data.load_npy_file(train_target_path)\n",
    "# data.input_val = data.load_npy_file(val_input_path)\n",
    "# data.target_val = data.load_npy_file(val_target_path)\n",
    "\n",
    "data.input_train = train_input\n",
    "data.target_train = train_target\n",
    "data.input_val = val_input\n",
    "data.target_val = val_target\n",
    "```\n",
    "\n",
    "3. Loading in scoring data\n",
    "\n",
    "```\n",
    "# CHANGE: COMMENTED OUT\n",
    "# # REPRODUCTION STEP: Needed to add path from current working directory to the parent directory with the scoring input and target files\n",
    "# data_path = '../../Project3-Resources/subsampled_low_res/'\n",
    "# scoring_input_path = data_path + \"scoring_input.npy\"\n",
    "# scoring_target_path = data_path + \"scoring_target.npy\"\n",
    "\n",
    "# # path to target input\n",
    "# data.input_scoring = np.load(scoring_input_path)\n",
    "\n",
    "# # path to target output\n",
    "# data.target_scoring = np.load(scoring_target_path)\n",
    "\n",
    "data.input_scoring = scoring_input\n",
    "data.target_scoring = scoring_target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: UNCOMMENT IF NOT INSTALLED\n",
    "# pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the repository name and type\n",
    "repo_name = \"LEAP/subsampled_low_res\"\n",
    "repo_type = \"dataset\"  # This specifies that you are downloading from a dataset repository\n",
    "\n",
    "# Download files and load them\n",
    "scoring_input = np.load(hf_hub_download(repo_id=repo_name, filename=\"scoring_input.npy\", repo_type=repo_type))\n",
    "scoring_target = np.load(hf_hub_download(repo_id=repo_name, filename=\"scoring_target.npy\", repo_type=repo_type))\n",
    "train_input = np.load(hf_hub_download(repo_id=repo_name, filename=\"train_input.npy\", repo_type=repo_type))\n",
    "train_target = np.load(hf_hub_download(repo_id=repo_name, filename=\"train_target.npy\", repo_type=repo_type))\n",
    "val_input = np.load(hf_hub_download(repo_id=repo_name, filename=\"val_input.npy\", repo_type=repo_type))\n",
    "val_target = np.load(hf_hub_download(repo_id=repo_name, filename=\"val_target.npy\", repo_type=repo_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: COMMENTED OUT\n",
    "# data_path = '../../Project3-Resources/subsampled_low_res/'\n",
    "\n",
    "# train_input_path = data_path + 'train_input.npy'\n",
    "# train_target_path = data_path + 'train_target.npy'\n",
    "# val_input_path = data_path + 'val_input.npy'\n",
    "# val_target_path = data_path + 'val_target.npy'\n",
    "\n",
    "# data.input_train = data.load_npy_file(train_input_path)\n",
    "# data.target_train = data.load_npy_file(train_target_path)\n",
    "# data.input_val = data.load_npy_file(val_input_path)\n",
    "# data.target_val = data.load_npy_file(val_target_path)\n",
    "\n",
    "data.input_train = train_input\n",
    "data.target_train = train_target\n",
    "data.input_val = val_input\n",
    "data.target_val = val_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train constant prediction model\n",
    "\n",
    "$\\hat{y} = E[y_{train}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_model = data.target_train.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multiple linear regression model\n",
    "\n",
    "$\\beta = {(X_{train}^TX_{train})}^{-1}X_{train}^Ty_{train}$\n",
    "\n",
    "$\\hat{y} = X_{input}^T \\beta$\n",
    "\n",
    "where $X_{train}$ and $X_{input}$ correspond to the training data and the input data you would like to inference on, respectively. $X_{train}$ and $X_{input}$ both have a column of ones concatenated to the feature space for the bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adding bias unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.input_train\n",
    "bias_vector = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((X, bias_vector), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_weights = np.linalg.inv(X.transpose()@X)@X.transpose()@data.target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# train your model here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pressure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Prediction\n",
    "const_pred_val = np.repeat(const_model[np.newaxis, :], data.target_val.shape[0], axis = 0)\n",
    "print(const_pred_val.shape)\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X_val = data.input_val\n",
    "bias_vector_val = np.ones((X_val.shape[0], 1))\n",
    "X_val = np.concatenate((X_val, bias_vector_val), axis=1)\n",
    "mlr_pred_val = X_val@mlr_weights\n",
    "print(mlr_pred_val.shape)\n",
    "\n",
    "# Load your prediction here\n",
    "\n",
    "# Load predictions into data_utils object\n",
    "data.model_names = ['const', 'mlr'] # add names of your models here\n",
    "preds = [const_pred_val, mlr_pred_val] # add your custom predictions here\n",
    "data.preds_val = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight predictions and target\n",
    "\n",
    "1. Undo output scaling\n",
    "\n",
    "2.  Weight vertical levels by dp/g\n",
    "\n",
    "3. Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "\n",
    "4. Convert units to a common energy unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reweight_target(data_split = 'val')\n",
    "data.reweight_preds(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_val\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained models with different hyperparameters, use the ones that performed the best on validation data for evaluation on scoring data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on scoring data\n",
    "\n",
    "#### Do this at the VERY END (when you have finished tuned the hyperparameters for your  model and are seeking a final evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: COMMENTED OUT\n",
    "# # REPRODUCTION STEP: Needed to add path from current working directory to the parent directory with the scoring input and target files\n",
    "# data_path = '../../Project3-Resources/subsampled_low_res/'\n",
    "# scoring_input_path = data_path + \"scoring_input.npy\"\n",
    "# scoring_target_path = data_path + \"scoring_target.npy\"\n",
    "\n",
    "# # path to target input\n",
    "# data.input_scoring = np.load(scoring_input_path)\n",
    "\n",
    "# # path to target output\n",
    "# data.target_scoring = np.load(scoring_target_path)\n",
    "\n",
    "data.input_scoring = scoring_input\n",
    "data.target_scoring = scoring_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pressure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant prediction\n",
    "const_pred_scoring = np.repeat(const_model[np.newaxis, :], data.target_scoring.shape[0], axis = 0)\n",
    "print(const_pred_scoring.shape)\n",
    "\n",
    "# multiple linear regression\n",
    "X_scoring = data.input_scoring\n",
    "bias_vector_scoring = np.ones((X_scoring.shape[0], 1))\n",
    "X_scoring = np.concatenate((X_scoring, bias_vector_scoring), axis=1)\n",
    "mlr_pred_scoring = X_scoring@mlr_weights\n",
    "print(mlr_pred_scoring.shape)\n",
    "\n",
    "# Your model prediction here\n",
    "\n",
    "# Load predictions into object\n",
    "data.model_names = ['const', 'mlr'] # model name here\n",
    "preds = [const_pred_scoring, mlr_pred_scoring] # add prediction here\n",
    "data.preds_scoring = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight predictions and target\n",
    "\n",
    "1. Undo output scaling\n",
    "\n",
    "2.  Weight vertical levels by dp/g\n",
    "\n",
    "3. Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "\n",
    "4. Convert units to a common energy unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight predictions and target\n",
    "data.reweight_target(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight predictions and target\n",
    "data.reweight_preds(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and calculate metrics\n",
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_scoring\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
